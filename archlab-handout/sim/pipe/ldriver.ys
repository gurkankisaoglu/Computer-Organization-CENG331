#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
#
# Gürkan Kısaoğlu
# 2171726
#
# Describe how and why you modified the baseline code.
#
# First i replaced all addq instructions with iaddq instruction since it combines
# two instructions(irmovq-addq) and increases performance.
# Second i implemented a loop unrolling which copies 10 elements in each turn, then i
# handled the remaining offset part with a jump table.
#
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
ncopy:
	iaddq $-9, %rdx 
	jle Offset

Loop: 
	mrmovq (%rdi), %r8 
	mrmovq 8(%rdi), %r9 
	mrmovq 16(%rdi), %r10 
	mrmovq 24(%rdi), %r11 
	mrmovq 32(%rdi), %r12
	mrmovq 40(%rdi), %r13
	mrmovq 48(%rdi), %r14
	mrmovq 64(%rdi), %rcx
	rmmovq %r8, (%rsi)
	andq %r8, %r8 
	jle Node1 
	iaddq $1, %rax 

Node1:
	mrmovq 72(%rdi), %r8
	rmmovq %r9, 8(%rsi) 
	andq %r9, %r9 
	jle Node2
	iaddq $1, %rax

Node2:
	mrmovq 56(%rdi), %r9 
	rmmovq %r10, 16(%rsi) 
	andq %r10, %r10 
	jle Node3
	iaddq $1, %rax

Node3:
	rmmovq %r11, 24(%rsi) 
	andq %r11, %r11
	jle Node4
	iaddq $1, %rax

Node4:
	rmmovq %r12, 32(%rsi)
	andq %r12, %r12 
	jle Node5
	iaddq $1, %rax

Node5:
	rmmovq %r13, 40(%rsi)
	andq %r13, %r13
	jle Node6
	iaddq $1, %rax

Node6:
	rmmovq %r14, 48(%rsi)
	andq %r14, %r14
	jle Node7
	iaddq $1, %rax

Node7:
	rmmovq %r9, 56(%rsi)
	andq %r9, %r9
	jle Node8
	iaddq $1, %rax

Node8:
	rmmovq %rcx, 64(%rsi)
	andq %rcx, %rcx
	jle Node9
	iaddq $1, %rax

Node9:
	rmmovq %r8, 72(%rsi)
	andq %r8, %r8
	jle Loop_Control
	iaddq $1, %rax

Loop_Control:
	iaddq $80, %rdi 
	iaddq $80, %rsi 
	iaddq $-10, %rdx 
	jg Loop 

Offset:
	addq %rdx, %rdx 
	addq %rdx, %rdx 
	addq %rdx, %rdx 
	mrmovq PostTable(%rdx), %rdx 
	mrmovq (%rdi), %r8 
	mrmovq 8(%rdi), %r9
	pushq %rdx 
	ret 

#.align 8
	.quad Done
	.quad offset1
	.quad offset2
	.quad offset3
	.quad offset4
	.quad offset5
	.quad offset6
	.quad offset7
	.quad offset8
PostTable:
	.quad offset9


offset9: 
	mrmovq 64(%rdi), %r11 
	mrmovq 56(%rdi), %r10
	rmmovq %r11, 64(%rsi) 
	andq %r11, %r11
	jle offset8_f
	iaddq $1, %rax

offset8: 
	mrmovq 56(%rdi), %r10 
offset8_f:
	mrmovq 48(%rdi), %r11 
	rmmovq %r10, 56(%rsi) 
	andq %r10, %r10 
	jle offset7_f
	iaddq $1, %rax

offset7:
	mrmovq 48(%rdi), %r11
offset7_f:
	mrmovq 40(%rdi), %r10
	rmmovq %r11, 48(%rsi)
	andq %r11, %r11
	jle offset6_f
	iaddq $1, %rax

offset6:
	mrmovq 40(%rdi), %r10
offset6_f:
	mrmovq 32(%rdi), %r11
	rmmovq %r10, 40(%rsi)
	andq %r10, %r10
	jle offset5_f
	iaddq $1, %rax

offset5:
	mrmovq 32(%rdi), %r11
offset5_f:
	mrmovq 24(%rdi), %r10
	rmmovq %r11, 32(%rsi)
	andq %r11, %r11
	jle offset4_f
	iaddq $1, %rax

offset4:
	mrmovq 24(%rdi), %r10
offset4_f:
	mrmovq 16(%rdi), %r11
	rmmovq %r10, 24(%rsi)
	andq %r10, %r10
	jle offset3_f
	iaddq $1, %rax

offset3:
	mrmovq 16(%rdi), %r11
offset3_f:
	rmmovq %r11, 16(%rsi)
	andq %r11, %r11
	jle offset2
	iaddq $1, %rax

offset2:
	rmmovq %r9, 8(%rsi)
	andq %r9, %r9
	jle offset1
	iaddq $1, %rax

offset1:
	rmmovq %r8, (%rsi)
	andq %r8, %r8
	jle 0x031
	iaddq $1, %rax

##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad -2
	.quad -3
	.quad -4
	.quad -5
	.quad 6
	.quad -7
	.quad 8
	.quad -9
	.quad -10
	.quad -11
	.quad -12
	.quad 13
	.quad 14
	.quad -15
	.quad -16
	.quad -17
	.quad 18
	.quad 19
	.quad 20
	.quad -21
	.quad -22
	.quad -23
	.quad 24
	.quad 25
	.quad 26
	.quad -27
	.quad -28
	.quad 29
	.quad -30
	.quad -31
	.quad 32
	.quad -33
	.quad 34
	.quad -35
	.quad 36
	.quad -37
	.quad 38
	.quad -39
	.quad 40
	.quad 41
	.quad 42
	.quad -43
	.quad -44
	.quad -45
	.quad 46
	.quad -47
	.quad 48
	.quad -49
	.quad -50
	.quad 51
	.quad -52
	.quad 53
	.quad -54
	.quad -55
	.quad 56
	.quad 57
	.quad 58
	.quad 59
	.quad 60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
